---
title: "Project3"
author: "Kelley Breeze and Chuanni He"
date: "2022-10-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Introduction  

This report contains a set of predictive models with automating mechanism. The data to be analyzed is the Online News Popularity Data Set summarizing a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity). The dataset contains 39,644 observations with 61 variables.  
In this project, we selected a subset of the variables as the predictor. Detailed descriptions for the predicting variables are listed below.
* `n_tokens_title`: Number of words in the title.
* `n_tokens_content`: Number of words in the content.
* `n_unique_tokens`: Rate of unique words in the content.
* `average_token_length`: Average length of the words in the content.
* `num_keywords`: Number of keywords in the metadata.
* `num_hrefs`: Number of links.
* `num_imgs`: Number of images.
* `num_videos`: Number of videos.
* `kw_min_min`: Worst keyword (min. shares).
* `kw_max_min`: Worst keyword (max. shares).
* `self_reference_min_shares`: Min. shares of referenced articles in Mashable.
* `self_reference_max_shares`: Max. shares of referenced articles in Mashable.
* `weekday_is_friday`: Was the article published on a Friday?
* `weekday_is_saturday`: Was the article published on a Saturday?
* `global_rate_positive_words`: Rate of positive words in the content.
* `global_rate_negative_words`: Rate of negative words in the content.  

The target variables is the `shares` variable indicating number of shares (target). The purpose of this project is to identify the optimal predicting variables and methods to predict the number of shares. This project applied statistical prediction models such as linear regression,  random forest model, and boosted tree model.
 

#### Packages Used  

```{r}
library(tidyverse)
library(caret)
```


# Data  

The code below uses a relative file path to import our data.  

```{r}
onlineNewsData<-read_csv('OnlineNewsPopularity.csv')
```

# Subset the data

We create a new variable `data_channel` representing all data chennels, and remove the six data channel variables.

```{r}
data_channel = rep(NA,nrow(onlineNewsData[,1]))
for (i in 1:nrow(onlineNewsData[,1])) {
  if (onlineNewsData$data_channel_is_lifestyle[i]==1) {data_channel[i]="lifestyle"} 
  else if (onlineNewsData$data_channel_is_entertainment[i]==1) {data_channel[i]="entertainment"}
  else if (onlineNewsData$data_channel_is_bus[i]==1) {data_channel[i]="bus"}
  else if (onlineNewsData$data_channel_is_socmed[i]==1) {data_channel[i]="socmed"}
  else if (onlineNewsData$data_channel_is_tech[i]==1) {data_channel[i]="tech"}
  else if (onlineNewsData$data_channel_is_world[i]==1) {data_channel[i]="world"}
  else {data_channel[i]="NotApplicable"}
}
onlineNewsData = onlineNewsData[,-c(14:19)]
onlineNewsData = cbind(onlineNewsData,data_channel)
```












#### `channelSelect()` Helper Function  

Next, we will subset our data to work on the first channel, `data_channel_is_lifestyle`.  We will create a helper function called `channelSelect()` to easily subset our data for different channels as desired.  

```{r}
## I do not think this is the one for automating. Please see the email, thanks!
channelSelect<- function(channel){
  onlineNewsData%>%
    select((paste0('data_channel_is_',channel)) | !starts_with('data_channel_is'))
}
```


Using `channelSelect()` function to select only information about the `data_channel_is_lifestyle` variable.  

```{r}
## I do not think this is the one for automating. Please see the email, thanks!
lifestyleChannelData<-channelSelect('lifestyle')
```




# Summarizations  

**You should produce some basic (but meaningful) summary statistics and plots about the training data you are working with (especially as it relates to your response).**  

**As you will automate this same analysis across other data, you can’t describe the trends you see in the graph (unless you want to try to automate that!). You should describe what to look for in the summary statistics/plots to help the reader understand the summary or graph. Ex: A scatter plot with the number of shares on the y-axis and the positive word rate on the x-axis is created:** 

**'We can inspect the trend of shares as a function of the positive word rate. If the points show an upward trend, then articles with more positive words tend to be shared more often. If we see a negative trend then articles with more positive words tend to be shared less often.'**

**Each group member is responsible for producing some summary statistics (means, sds, contingency tables, etc.) and for producing at least three graphs (each) of the data. ** 


# Modeling  

#### Spliting the Data  

**You’ll need to split the data into a training (70% of the data) and test set (30% of the data). Use set.seed() to make things reproducible. The goal is to create models for predicting the number of shares in some way. Each group member should contribute a linear regression model and an ensemble tree-based model. As we are automating things, describing the chosen model is tough, so no need to worry about that.**


The `splitDataFunction()` will split our dataset into a `newsTrain` set (containing 70% of our data) and a `newsTest` set (containing 30% of our data).  

```{r}
splitDataFunction<- function(datasetChannel){
# set.seed for reproducibility
  set.seed(1234)
#Creating our training index and splitting our data.
  trainIndex<- createDataPartition(datasetChannel$timedelta, p = 0.70, list=FALSE)
  newsTrain<- datasetChannel[trainIndex,]
  newsTest<- datasetChannel[-trainIndex,]
  
  return(list(newsTrain, newsTest))
}
```

```{r}
splitLifestyle<-splitDataFunction(lifestyleChannelData)
```

## Random Forest Model  

**The first group member should fit a random forest model. Prior to each ensemble model, you should provide a short but reasonably thorough explanation of the ensemble model you are using (so one for each group member).**


## Boosted Tree Model  

**The second group member should fit a boosted tree model. Prior to each ensemble model, you should provide a short but reasonably thorough explanation of the ensemble model you are using (so one for each group member).**

## Linear Regression Models  

**Prior to the models fit using linear regression, the first group member should provide a short but thorough explanation of the idea of a linear regression model.**


# Comparison  

**All four of the models should be compared on the test set and a winner declared (this should be automated to be correct across all the created documents). This can be done by one group member and the automation done by the other (see below).**


# Automation  

**Once you’ve completed the above for a particular data channel, adapt the code so that you can use a parameter in your build process. You should be able to automatically generate an analysis report for each data_channel_is_* variable - although again, you may want to create a new variable to help with the subsetting. You’ll end up with six total outputted documents. This should be done by the group member that doesn’t automate the comparison of models part.**